# Services Architecture

This document provides a comprehensive overview of the microservices architecture used in the Trading Signals Reader AI Bot system. The system is designed using a modular, scalable microservices approach that enables independent development, deployment, and scaling of different components.

## ðŸ—ï¸ Architecture Overview

The system follows a **Domain-Driven Design (DDD)** approach with clear service boundaries and well-defined interfaces. Each service is responsible for a specific business domain and communicates with other services through well-defined APIs and message queues.

### Core Design Principles

1. **Single Responsibility**: Each service has a single, well-defined purpose
2. **Loose Coupling**: Services are independent and communicate through APIs
3. **High Cohesion**: Related functionality is grouped within the same service
4. **Fault Tolerance**: Services can handle failures gracefully
5. **Scalability**: Services can be scaled independently based on demand
6. **Observability**: Comprehensive logging, monitoring, and tracing

## ðŸ”§ Service Catalog

### 1. API Gateway Service

**Purpose**: Central entry point for all client requests, handling routing, authentication, rate limiting, and request/response transformation.

**Responsibilities**:
- Request routing to appropriate backend services
- JWT token validation and user authentication
- Rate limiting and throttling
- Request/response logging and monitoring
- API versioning and backward compatibility
- CORS handling
- Request validation and sanitization

**Technology Stack**:
- FastAPI with custom middleware
- Redis for rate limiting and caching
- JWT for authentication
- Prometheus metrics collection

**Key Endpoints**:
```
GET  /health                    # Health check
POST /auth/login               # User authentication
POST /auth/refresh             # Token refresh
GET  /api/v1/*                 # Proxied API requests
```

**Configuration**:
```yaml
api_gateway:
  port: 8000
  rate_limit:
    requests_per_minute: 1000
    burst_size: 100
  cors:
    allowed_origins: ["*"]
    allowed_methods: ["GET", "POST", "PUT", "DELETE"]
  jwt:
    secret_key: ${JWT_SECRET_KEY}
    algorithm: "HS256"
    access_token_expire_minutes: 30
```

### 2. User Management Service

**Purpose**: Handles user registration, authentication, profile management, and user-related operations.

**Responsibilities**:
- User registration and email verification
- Password management and reset
- User profile management
- User preferences and settings
- Role-based access control (RBAC)
- User session management
- Account security features (2FA, login history)

**Database Schema**:
```sql
-- Users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    username VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    is_active BOOLEAN DEFAULT true,
    is_verified BOOLEAN DEFAULT false,
    role VARCHAR(50) DEFAULT 'user',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP
);

-- User settings table
CREATE TABLE user_settings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    timezone VARCHAR(50) DEFAULT 'UTC',
    language VARCHAR(10) DEFAULT 'en',
    currency VARCHAR(10) DEFAULT 'USD',
    notifications_enabled BOOLEAN DEFAULT true,
    email_notifications BOOLEAN DEFAULT true,
    telegram_notifications BOOLEAN DEFAULT false,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**API Endpoints**:
```
POST /users/register           # User registration
POST /users/login              # User login
POST /users/logout             # User logout
GET  /users/profile            # Get user profile
PUT  /users/profile            # Update user profile
GET  /users/settings           # Get user settings
PUT  /users/settings           # Update user settings
POST /users/change-password    # Change password
POST /users/reset-password     # Reset password
```

### 3. Trading Service

**Purpose**: Manages all trading-related operations including order management, portfolio tracking, and exchange integrations.

**Responsibilities**:
- Order creation, modification, and cancellation
- Portfolio management and tracking
- Position management
- Trade execution and monitoring
- Risk management and validation
- Exchange API integration
- Trading history and reporting

**Core Components**:

#### Order Management Engine
```python
class OrderManager:
    def create_order(self, order_request: OrderRequest) -> Order:
        # Validate order parameters
        # Check user balance and limits
        # Apply risk management rules
        # Submit to exchange
        # Store in database
        # Send notifications
        pass
    
    def cancel_order(self, order_id: str) -> bool:
        # Cancel order on exchange
        # Update order status
        # Send notifications
        pass
    
    def get_order_status(self, order_id: str) -> OrderStatus:
        # Fetch order status from exchange
        # Update local database
        # Return current status
        pass
```

#### Portfolio Manager
```python
class PortfolioManager:
    def get_portfolio(self, user_id: str) -> Portfolio:
        # Aggregate positions across exchanges
        # Calculate total value and P&L
        # Apply real-time price updates
        pass
    
    def calculate_metrics(self, portfolio: Portfolio) -> PortfolioMetrics:
        # Calculate performance metrics
        # Risk metrics (VaR, Sharpe ratio)
        # Allocation analysis
        pass
```

**Database Schema**:
```sql
-- Trading pairs table
CREATE TABLE trading_pairs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    symbol VARCHAR(20) NOT NULL,
    base_asset VARCHAR(10) NOT NULL,
    quote_asset VARCHAR(10) NOT NULL,
    exchange VARCHAR(50) NOT NULL,
    is_active BOOLEAN DEFAULT true,
    min_order_size DECIMAL(20, 8),
    max_order_size DECIMAL(20, 8),
    price_precision INTEGER,
    quantity_precision INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Orders table
CREATE TABLE orders (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    exchange_order_id VARCHAR(100),
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL, -- 'buy' or 'sell'
    type VARCHAR(20) NOT NULL, -- 'market', 'limit', 'stop_loss'
    quantity DECIMAL(20, 8) NOT NULL,
    price DECIMAL(20, 8),
    stop_price DECIMAL(20, 8),
    status VARCHAR(20) DEFAULT 'pending',
    filled_quantity DECIMAL(20, 8) DEFAULT 0,
    average_price DECIMAL(20, 8),
    commission DECIMAL(20, 8),
    commission_asset VARCHAR(10),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    executed_at TIMESTAMP
);

-- Portfolios table
CREATE TABLE portfolios (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    is_default BOOLEAN DEFAULT false,
    total_value DECIMAL(20, 8) DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Positions table
CREATE TABLE positions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    portfolio_id UUID REFERENCES portfolios(id),
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(50) NOT NULL,
    quantity DECIMAL(20, 8) NOT NULL,
    average_price DECIMAL(20, 8) NOT NULL,
    current_price DECIMAL(20, 8),
    unrealized_pnl DECIMAL(20, 8),
    realized_pnl DECIMAL(20, 8) DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 4. AI Service

**Purpose**: Provides AI-powered features including natural language processing, trading signal generation, and market analysis.

**Responsibilities**:
- Natural language command processing
- Trading signal generation using ML models
- Market sentiment analysis
- Price prediction and forecasting
- Pattern recognition in charts
- Risk assessment and scoring
- AI model training and deployment

**Core Components**:

#### NLP Engine
```python
class NLPEngine:
    def __init__(self):
        self.intent_classifier = load_model('intent_classifier')
        self.entity_extractor = load_model('entity_extractor')
        self.gpt_client = OpenAI(api_key=settings.OPENAI_API_KEY)
    
    def process_command(self, text: str, user_context: dict) -> CommandResult:
        # Extract intent and entities
        intent = self.classify_intent(text)
        entities = self.extract_entities(text)
        
        # Generate response using GPT-4
        response = self.generate_response(intent, entities, user_context)
        
        return CommandResult(
            intent=intent,
            entities=entities,
            response=response,
            actions=self.determine_actions(intent, entities)
        )
```

#### Signal Generation Engine
```python
class SignalGenerator:
    def __init__(self):
        self.lstm_model = load_model('price_prediction_lstm')
        self.sentiment_model = load_model('sentiment_analysis')
        self.pattern_model = load_model('pattern_recognition')
    
    def generate_signals(self, symbol: str, timeframe: str) -> TradingSignals:
        # Get market data
        market_data = self.get_market_data(symbol, timeframe)
        
        # Technical analysis
        technical_signals = self.analyze_technical_indicators(market_data)
        
        # ML-based predictions
        price_prediction = self.predict_price(market_data)
        
        # Sentiment analysis
        sentiment_score = self.analyze_sentiment(symbol)
        
        # Pattern recognition
        patterns = self.detect_patterns(market_data)
        
        # Combine signals
        return self.combine_signals(
            technical_signals,
            price_prediction,
            sentiment_score,
            patterns
        )
```

**Database Schema**:
```sql
-- AI commands table
CREATE TABLE ai_commands (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    command_text TEXT NOT NULL,
    intent VARCHAR(50),
    entities JSONB,
    response TEXT,
    status VARCHAR(20) DEFAULT 'processing',
    confidence_score DECIMAL(5, 4),
    processing_time_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP
);

-- Trading signals table
CREATE TABLE trading_signals (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    symbol VARCHAR(20) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,
    signal_type VARCHAR(20) NOT NULL, -- 'buy', 'sell', 'hold'
    strength DECIMAL(5, 4) NOT NULL, -- 0.0 to 1.0
    confidence DECIMAL(5, 4) NOT NULL,
    price_target DECIMAL(20, 8),
    stop_loss DECIMAL(20, 8),
    take_profit DECIMAL(20, 8),
    reasoning TEXT,
    technical_indicators JSONB,
    ml_predictions JSONB,
    sentiment_score DECIMAL(5, 4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP
);

-- Market analysis table
CREATE TABLE market_analysis (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    symbol VARCHAR(20) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,
    analysis_type VARCHAR(50) NOT NULL,
    summary TEXT,
    key_levels JSONB,
    support_resistance JSONB,
    trend_analysis JSONB,
    volatility_analysis JSONB,
    risk_assessment JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 5. Market Data Service

**Purpose**: Aggregates and processes real-time and historical market data from multiple exchanges and data providers.

**Responsibilities**:
- Real-time price data collection
- Historical data management
- Technical indicator calculations
- Market depth and order book data
- News and sentiment data aggregation
- Data normalization and validation
- Caching and performance optimization

**Core Components**:

#### Data Aggregator
```python
class MarketDataAggregator:
    def __init__(self):
        self.exchanges = {
            'binance': BinanceConnector(),
            'coinbase': CoinbaseConnector(),
            'kraken': KrakenConnector()
        }
        self.redis_client = Redis()
    
    async def stream_real_time_data(self, symbols: List[str]):
        # Start WebSocket connections to exchanges
        # Normalize and validate data
        # Cache in Redis
        # Broadcast to subscribers
        pass
    
    def get_historical_data(self, symbol: str, timeframe: str, limit: int) -> DataFrame:
        # Check cache first
        # Fetch from exchanges if needed
        # Store in database
        # Return normalized data
        pass
```

#### Technical Analysis Engine
```python
class TechnicalAnalysisEngine:
    def calculate_indicators(self, data: DataFrame) -> Dict[str, Any]:
        indicators = {}
        
        # Moving averages
        indicators['sma_20'] = ta.SMA(data['close'], timeperiod=20)
        indicators['ema_20'] = ta.EMA(data['close'], timeperiod=20)
        
        # Oscillators
        indicators['rsi'] = ta.RSI(data['close'], timeperiod=14)
        indicators['macd'], indicators['macd_signal'], indicators['macd_hist'] = ta.MACD(data['close'])
        
        # Volatility
        indicators['bb_upper'], indicators['bb_middle'], indicators['bb_lower'] = ta.BBANDS(data['close'])
        
        # Volume
        indicators['volume_sma'] = ta.SMA(data['volume'], timeperiod=20)
        
        return indicators
```

**Database Schema**:
```sql
-- Market data table (time-series)
CREATE TABLE market_data (
    id BIGSERIAL PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(50) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    open_price DECIMAL(20, 8) NOT NULL,
    high_price DECIMAL(20, 8) NOT NULL,
    low_price DECIMAL(20, 8) NOT NULL,
    close_price DECIMAL(20, 8) NOT NULL,
    volume DECIMAL(20, 8) NOT NULL,
    quote_volume DECIMAL(20, 8),
    trade_count INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Create indexes for efficient querying
CREATE INDEX idx_market_data_symbol_timeframe_timestamp 
ON market_data (symbol, timeframe, timestamp DESC);

-- Technical indicators table
CREATE TABLE technical_indicators (
    id BIGSERIAL PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    indicator_name VARCHAR(50) NOT NULL,
    value DECIMAL(20, 8),
    additional_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Order book data table
CREATE TABLE order_book_snapshots (
    id BIGSERIAL PRIMARY KEY,
    symbol VARCHAR(20) NOT NULL,
    exchange VARCHAR(50) NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    bids JSONB NOT NULL, -- Array of [price, quantity]
    asks JSONB NOT NULL, -- Array of [price, quantity]
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 6. Notification Service

**Purpose**: Handles all types of notifications including email, Telegram, push notifications, and in-app alerts.

**Responsibilities**:
- Email notifications (SMTP)
- Telegram bot notifications
- Push notifications (mobile)
- In-app notifications
- Notification preferences management
- Template management
- Delivery tracking and retry logic

**Core Components**:

#### Notification Manager
```python
class NotificationManager:
    def __init__(self):
        self.email_service = EmailService()
        self.telegram_service = TelegramService()
        self.push_service = PushNotificationService()
    
    async def send_notification(self, notification: NotificationRequest):
        # Determine delivery channels based on user preferences
        channels = self.get_user_notification_channels(notification.user_id)
        
        # Send via each enabled channel
        tasks = []
        for channel in channels:
            if channel == 'email':
                tasks.append(self.email_service.send(notification))
            elif channel == 'telegram':
                tasks.append(self.telegram_service.send(notification))
            elif channel == 'push':
                tasks.append(self.push_service.send(notification))
        
        # Execute all notifications concurrently
        await asyncio.gather(*tasks, return_exceptions=True)
```

**Database Schema**:
```sql
-- Notifications table
CREATE TABLE notifications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    type VARCHAR(50) NOT NULL, -- 'trade_executed', 'price_alert', 'system'
    title VARCHAR(200) NOT NULL,
    message TEXT NOT NULL,
    channels VARCHAR(100)[], -- ['email', 'telegram', 'push']
    status VARCHAR(20) DEFAULT 'pending',
    priority VARCHAR(20) DEFAULT 'normal', -- 'low', 'normal', 'high', 'urgent'
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    sent_at TIMESTAMP,
    read_at TIMESTAMP
);

-- Notification preferences table
CREATE TABLE notification_preferences (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    notification_type VARCHAR(50) NOT NULL,
    email_enabled BOOLEAN DEFAULT true,
    telegram_enabled BOOLEAN DEFAULT false,
    push_enabled BOOLEAN DEFAULT true,
    in_app_enabled BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 7. Telegram Bot Service

**Purpose**: Provides a conversational interface for trading operations through Telegram.

**Responsibilities**:
- Command processing and routing
- Natural language understanding
- User session management
- Interactive keyboards and menus
- File and media handling
- Webhook management
- Bot analytics and monitoring

**Core Components**:

#### Bot Handler
```python
class TelegramBotHandler:
    def __init__(self):
        self.bot = Bot(token=settings.TELEGRAM_BOT_TOKEN)
        self.ai_service = AIService()
        self.trading_service = TradingService()
    
    async def handle_message(self, update: Update, context: CallbackContext):
        user_id = update.effective_user.id
        message_text = update.message.text
        
        # Process with AI service
        ai_response = await self.ai_service.process_command(
            text=message_text,
            user_context={'telegram_user_id': user_id}
        )
        
        # Execute actions if any
        if ai_response.actions:
            await self.execute_actions(ai_response.actions, user_id)
        
        # Send response
        await update.message.reply_text(ai_response.response)
```

**Database Schema**:
```sql
-- Telegram users table
CREATE TABLE telegram_users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    telegram_user_id BIGINT UNIQUE NOT NULL,
    username VARCHAR(100),
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    language_code VARCHAR(10),
    is_bot BOOLEAN DEFAULT false,
    is_active BOOLEAN DEFAULT true,
    chat_id BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_interaction TIMESTAMP
);

-- Telegram commands table
CREATE TABLE telegram_commands (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    telegram_user_id BIGINT NOT NULL,
    command TEXT NOT NULL,
    response TEXT,
    status VARCHAR(20) DEFAULT 'completed',
    processing_time_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ðŸ”„ Service Communication

### Synchronous Communication (REST APIs)

Services communicate synchronously for immediate responses and critical operations:

```python
# Example: Trading service calling AI service
class TradingService:
    def __init__(self):
        self.ai_service_client = HTTPClient(base_url="http://ai-service:8001")
    
    async def validate_order_with_ai(self, order_request: OrderRequest) -> bool:
        response = await self.ai_service_client.post(
            "/risk-assessment",
            json=order_request.dict()
        )
        return response.json()["is_safe"]
```

### Asynchronous Communication (Message Queues)

For non-critical operations and event-driven workflows:

```python
# Example: Publishing order execution event
class OrderExecutor:
    def __init__(self):
        self.message_broker = MessageBroker()
    
    async def execute_order(self, order: Order):
        # Execute the order
        result = await self.exchange_client.place_order(order)
        
        # Publish event for other services
        await self.message_broker.publish(
            topic="order.executed",
            message={
                "order_id": order.id,
                "user_id": order.user_id,
                "symbol": order.symbol,
                "status": result.status,
                "executed_at": datetime.utcnow().isoformat()
            }
        )
```

### Event-Driven Architecture

Key events and their subscribers:

```yaml
events:
  user.registered:
    subscribers:
      - notification_service  # Send welcome email
      - trading_service      # Create default portfolio
      - telegram_service     # Setup bot integration
  
  order.executed:
    subscribers:
      - notification_service  # Send execution notification
      - portfolio_service    # Update portfolio
      - ai_service          # Update ML models
  
  price.alert:
    subscribers:
      - notification_service  # Send price alert
      - telegram_service     # Send Telegram message
  
  market.data.updated:
    subscribers:
      - ai_service          # Update predictions
      - trading_service     # Check stop losses
      - notification_service # Check price alerts
```

## ðŸ” Service Discovery and Load Balancing

### Service Registry

Services register themselves with a service registry for discovery:

```python
class ServiceRegistry:
    def __init__(self):
        self.redis_client = Redis()
    
    async def register_service(self, service_name: str, instance_info: dict):
        key = f"services:{service_name}"
        await self.redis_client.hset(key, mapping=instance_info)
        await self.redis_client.expire(key, 30)  # TTL for health checking
    
    async def discover_service(self, service_name: str) -> List[dict]:
        pattern = f"services:{service_name}:*"
        keys = await self.redis_client.keys(pattern)
        instances = []
        for key in keys:
            instance = await self.redis_client.hgetall(key)
            instances.append(instance)
        return instances
```

### Load Balancing

Round-robin load balancing with health checks:

```python
class LoadBalancer:
    def __init__(self, service_registry: ServiceRegistry):
        self.service_registry = service_registry
        self.current_index = {}
    
    async def get_service_instance(self, service_name: str) -> dict:
        instances = await self.service_registry.discover_service(service_name)
        healthy_instances = await self.filter_healthy_instances(instances)
        
        if not healthy_instances:
            raise ServiceUnavailableError(f"No healthy instances of {service_name}")
        
        # Round-robin selection
        index = self.current_index.get(service_name, 0)
        instance = healthy_instances[index % len(healthy_instances)]
        self.current_index[service_name] = index + 1
        
        return instance
```

## ðŸ“Š Monitoring and Observability

### Health Checks

Each service implements health check endpoints:

```python
@app.get("/health")
async def health_check():
    checks = {
        "database": await check_database_connection(),
        "redis": await check_redis_connection(),
        "external_apis": await check_external_apis()
    }
    
    overall_status = "healthy" if all(checks.values()) else "unhealthy"
    
    return {
        "status": overall_status,
        "timestamp": datetime.utcnow().isoformat(),
        "checks": checks,
        "version": app.version
    }
```

### Metrics Collection

Prometheus metrics for monitoring:

```python
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
request_duration = Histogram('http_request_duration_seconds', 'HTTP request duration')
active_connections = Gauge('active_connections', 'Number of active connections')

# Middleware to collect metrics
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    request_duration.observe(duration)
    
    return response
```

### Distributed Tracing

OpenTelemetry integration for request tracing:

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Use in service methods
async def process_order(order_request: OrderRequest):
    with tracer.start_as_current_span("process_order") as span:
        span.set_attribute("order.symbol", order_request.symbol)
        span.set_attribute("order.side", order_request.side)
        
        # Process order logic
        result = await execute_order_logic(order_request)
        
        span.set_attribute("order.status", result.status)
        return result
```

## ðŸ”’ Security Considerations

### Service-to-Service Authentication

JWT tokens for service authentication:

```python
class ServiceAuthenticator:
    def __init__(self, secret_key: str):
        self.secret_key = secret_key
    
    def generate_service_token(self, service_name: str) -> str:
        payload = {
            "service": service_name,
            "iat": datetime.utcnow(),
            "exp": datetime.utcnow() + timedelta(hours=1)
        }
        return jwt.encode(payload, self.secret_key, algorithm="HS256")
    
    def verify_service_token(self, token: str) -> dict:
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=["HS256"])
            return payload
        except jwt.InvalidTokenError:
            raise AuthenticationError("Invalid service token")
```

### Network Security

- **TLS/SSL**: All service-to-service communication uses TLS
- **Network Segmentation**: Services run in isolated network segments
- **Firewall Rules**: Strict firewall rules between service networks
- **VPN**: Secure VPN for administrative access

### Data Protection

- **Encryption at Rest**: Database encryption for sensitive data
- **Encryption in Transit**: TLS for all network communication
- **Secret Management**: HashiCorp Vault for secret storage
- **Data Masking**: Sensitive data masking in logs

## ðŸš€ Deployment Strategy

### Container Orchestration

Kubernetes deployment with proper resource allocation:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trading-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: trading-service
  template:
    metadata:
      labels:
        app: trading-service
    spec:
      containers:
      - name: trading-service
        image: trading-bot/trading-service:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

### Auto-scaling

Horizontal Pod Autoscaler configuration:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trading-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ðŸ“ˆ Performance Optimization

### Caching Strategy

```python
class CacheManager:
    def __init__(self):
        self.redis_client = Redis()
        self.local_cache = TTLCache(maxsize=1000, ttl=300)
    
    async def get_with_cache(self, key: str, fetch_func, ttl: int = 300):
        # Try local cache first
        if key in self.local_cache:
            return self.local_cache[key]
        
        # Try Redis cache
        cached_value = await self.redis_client.get(key)
        if cached_value:
            value = json.loads(cached_value)
            self.local_cache[key] = value
            return value
        
        # Fetch from source
        value = await fetch_func()
        
        # Cache in both layers
        await self.redis_client.setex(key, ttl, json.dumps(value))
        self.local_cache[key] = value
        
        return value
```

### Database Optimization

```sql
-- Optimized indexes for common queries
CREATE INDEX CONCURRENTLY idx_orders_user_status_created 
ON orders (user_id, status, created_at DESC);

CREATE INDEX CONCURRENTLY idx_market_data_symbol_timestamp 
ON market_data (symbol, timestamp DESC) 
WHERE timeframe = '1m';

-- Partitioning for large tables
CREATE TABLE market_data_2024 PARTITION OF market_data
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

### Connection Pooling

```python
class DatabaseManager:
    def __init__(self):
        self.engine = create_async_engine(
            DATABASE_URL,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        self.session_factory = async_sessionmaker(
            self.engine,
            expire_on_commit=False
        )
```

## ðŸ”„ Data Flow and Integration Patterns

### Event Sourcing

For critical business events:

```python
class EventStore:
    async def append_event(self, stream_id: str, event: DomainEvent):
        event_data = {
            "stream_id": stream_id,
            "event_type": event.__class__.__name__,
            "event_data": event.dict(),
            "version": await self.get_next_version(stream_id),
            "timestamp": datetime.utcnow()
        }
        
        await self.db.execute(
            "INSERT INTO events (stream_id, event_type, event_data, version, timestamp) "
            "VALUES (:stream_id, :event_type, :event_data, :version, :timestamp)",
            event_data
        )
```

### CQRS (Command Query Responsibility Segregation)

```python
# Command side - for writes
class CreateOrderCommand:
    def __init__(self, order_request: OrderRequest):
        self.order_request = order_request

class CreateOrderHandler:
    async def handle(self, command: CreateOrderCommand):
        # Validate command
        # Create order aggregate
        # Save events
        # Publish domain events
        pass

# Query side - for reads
class OrderQueryService:
    async def get_user_orders(self, user_id: str, filters: dict) -> List[Order]:
        # Optimized read model queries
        # Return denormalized data
        pass
```

This services architecture provides a robust, scalable foundation for the Trading Signals Reader AI Bot, enabling independent development, deployment, and scaling of different system components while maintaining clear boundaries and communication patterns.